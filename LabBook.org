#+TITLE: LabBook
#+AUTHOR: Anderson Mattheus Maliszewski
#+STARTUP: overview indent
#+TAGS: noexport(n) deprecated(d) 
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

This LabBook is for the *CMP223 - Computer System Performance* *Analysis
(2019/2)* final work and has the step-by-step procedure used during the
proposed experiments. Also, to reproduce this evaluation, you can
follow the steps below. Have fun!

#+BEGIN_QUOTE
*Reporting errors*: This repository has several links to self-contained
 files as well as links from the Internet, so if you try to reproduce
 it and find broken links or other problems, please tell me everything
 so that I can improve it. :)
#+END_QUOTE

* Experimental Project
** Objective
   Assess the impact of the network interconnection on HPC
   applications. Both *InfiniBand (IB)* and *Gigabit Ethernet (ETH)*, as
   well as *IP-over-IB (IPoIB)* interconnections were evaluated using
   the same physical cluster of servers. The MPI-PingPong benchmark of
   the [[https://software.intel.com/en-us/articles/intel-mpi-benchmarks][Intel MPI Benchmarks]] suite was executed to first characterize
   interconnect latency and throughput. Then two synthetic benchmarks
   (NPB and ImbBench) and one real application (Alya) were
   executed. [[https://www.nas.nasa.gov/publications/npb.html][NAS Parallel Benchmarks (NPB)]] set version 3.4 with input
   *class D* was used because it represent several patterns of real HPC
   applications, as well as being characterized as a balanced
   benchmarks sed (when and MPI parallel application makes balanced
   use of processes and consequently they all end almost together). On
   the other hand, [[https://github.com/Roloff/ImbBench.git][Inbalanced Benchmark (IMB)]] was created to represent
   the more common inbalanced application patterns. Thus, with both of
   the mentioned benchmarks we cover the common HPC application and
   also the balanced and unbalanced patterns. [[https://www.bsc.es/research-development/research-areas/engineering-simulations/alya-high-performance-computational][Alya]] was used because
   this work tends to cover not only synthetic benchmarks which try to
   mimic real applications but rather use a real application and
   evaluate them. In the next session the benchmarks are described
   more detail.

** Benchmarks
Below the benchmarks used are described.
*** Intel MPI Benchmark
The IntelÂ® MPI Benchmarks perform a set of MPI performance
measurements for point-to-point and global communication operations
for a range of message sizes. The generated benchmark data fully
characterizes:
- Performance of a cluster system, including node performance, network
  latency, and throughput.
- Efficiency of the MPI implementation used.
There are several benchmarks included in this set, and in this
evaluation, only the MPI1 PingPong application was used to measure
interconnect latency and throughput.

*** NAS Parallel Benchmarks
The NAS Parallel Benchmarks (NPB) are a small set of programs designed
to help evaluate the performance of parallel supercomputers. The
benchmarks are derived from computational fluid dynamics (CFD)
applications. Problem sizes in NPB are predefined and indicated as
different classes.

In this evaluation, the original set of benchmarks from the NPB suite,
consisting of five kernels and three pseudo-applications were used
with the Messsage Passing Interface (MPI) parallel implementation.

Five Kernels:
- *IS* - Integer Sort, random memory access.
- *EP* - Embarrassingly Parallel.
- *CG* - Conjugate Gradient, irregular memory access and communication.
- *FT* - Discrete 3D fast Fourier Transform, all-to-all communication.
- *MG* - Multi-Grid on a sequence of meshes, long- and short-distance
  communication, memory intensive.

Three pseudo-applications: 
- *BT* - Block Tri-diagonal solver.
- *SP* - Scalar Penta-diagonal solver.
- *LU* - Lower-Upper Gauss-Seidel solver.

They were executed with 128 processes on 4 nodes, 32 processes per
node, in the case of IS, EP, CG, FT, MG, and LU (power-of-two). Since
BT and SP require the number of processes to be a square root, 144
processes were used, with 36 processes in each node.

*** Inbalanced Benchmark
Imbalance Benchmark (ImbBench) is a set of MPI-based applications,
created by the Ph.D. Student Eduardo Roloff, that simulate several
behaviors in terms of process loads. ImbBench was designed with the
heterogeneity of the cloud in mind, and its goal is to help the user
to choose the most suitable configuration to execute an application in
the cloud. ImbBench distributes the load among all the available
processes according to a preselected imbalance pattern.

ImbBench has a set of microbenchmarks and parameters to benchmark CPU
and Memory. In this evaluation, it was used both CPU and Memory
benchmarks with the 8Level pattern of imbalance and the Rand and BST
microbenchmarks, respectively.

*** Alya
Alya is a high performance computational mechanics code to solve
complex coupled multi-physics / multi-scale / multi-domain problems,
which are mostly coming from the engineering realm. Among the
different physics solved by Alya we can mention:
incompressible/compressible flows, non-linear solid mechanics,
chemistry, particle transport, heat transfer, turbulence modeling,
electrical propagation, etc.

** How to Reproduce it
To reproduce this project, the first step is to to clone the git repository in
the *$HOME* of the desired cluster of servers. 

#+begin_src shell :results output :exports both
cd $HOME; git clone https://github.com/andermm/CMP223
#+end_src

Here it is assumed that your HOME directory is exported with NFS. If
you don't have the NFS configured, [[https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04][here]] is a tutorial to do it. If
this step is ok, go ahead to the other topics.

** Software Installation
To execute the experiments, some required packages need to
be installed. They are:
- openmpi-bin - High performance message passing library (mpiexec).
- libopenmpi-dev - High performance message passing library (mpicc and
  mpifort).
- gfortran - GNU Fortran 95 compiler.
- cmake - Cross-platform, open-source make system.
- pajeng - Space-time view and associated tools for Paje trace files.

If your OS is Debian based and you have permission to install new
packages, you can install the required packages using the [[SH/software_install.sh][Software
Installer]] script. On the other hand, if you do not have permission to
do so, you can use the Spack package manager. The full tutorial on how
to use Spack can be seen [[https://spack-tutorial.readthedocs.io/en/latest/][here]].

** System Information 
   To collect the information of all nodes used in the evaluation, it
   was used the [[SH/sys_info_collect.sh][System Information Collect]] script, which creates the
   [[LOGS/env_info.org][System Information]] log output with ORG extension. The script
   is executes automatically in the execution script.

** Network Infrastructure Information
Each node has a Mellanox MT27600 Channel Adapter (CA) configured for
the InfiniBand 56 Gb/s 4X FDR ConnectX-3 with firmware version
10.16.1038 and OFED version 4.6-1.0.1.1. All nodes are interconnected
through a Mellanox SX6036 FDR and a generic de 1 Gbps switch.

** Design of Experiments
   To execute benchmarks without bias, the DoE.base library was used
   to create Experiment Design (DoE). In DoE, two factors were used,
   applications and interface, with 30 random replications, totaling
   1080 (12 * 3 * 30) distinct executions. For the characterization of
   the applications, one random replication was performed totaling 33
   (11 * 3) distinct executions (the MPI PingPong benchmark was not
   performed in the characterization step).

Execution factors:
- Factor 1 - Apps: The applications name totaling 12 (~exec_bt~,
  ~exec_ep~, ~exec_cg~, ~exec_mg~, ~exec_lu~, ~exec_sp~, ~exec_is~, ~exec_ft~,
  ~exec_imb_memory~, ~exec_imb_CPU~, ~exec_intel~, and ~exec_alya~).
- Factor 2 - Interface: The network interface name, totaling 3 (~eth~,
  ~ib~, ~ipoib~).

Characterization factors:
- Factor 1 - Apps: The applications name totaling 11 (~charac_bt~,
  ~charac_ep~, ~charac_cg~, ~charac_mg~, ~charac_lu~, ~charac_sp~, ~charac_is~,
  ~charac_ft~, ~charac_imb_memory~, ~charac_imb_CPU~, and ~charac_alya~).
- Factor 2 - Interface: The network interface name, totaling 3 (~eth~,
  ~ib~, ~ipoib~).

Below is the R block of code which generate the CSV files.
#+begin_src R :results output :session *R* :exports both
options(crayon.enabled=FALSE)
library(DoE.base)
library(tidyverse)

set.seed(0)
  cmp223_exec <- fac.design(factor.names = list(
    apps = c("exec_bt", "exec_ep", "exec_cg", "exec_mg", "exec_lu",
             "exec_sp", "exec_is", "exec_ft", "exec_imb_memory",
             "exec_imb_CPU", "exec_intel", "exec_alya"),
    interface = c("eth", "ib", "ipoib")),
    replications=30,
    randomize=TRUE)

cmp223_exec %>%
  select(-Blocks) %>%
  mutate(number=1:n()) -> cmp223_exec
write_csv(cmp223_exec, "R/experimental_project_exec.csv")
#+end_src

#+RESULTS:
: 
: creating full factorial with 36 runs ...

#+begin_src R :results output :session *R* :exports both
options(crayon.enabled=FALSE)
library(DoE.base)
library(tidyverse)

set.seed(0)
  cmp223_charac <- fac.design(factor.names = list(
    apps = c("charac_bt", "charac_ep", "charac_cg", "charac_mg", "charac_lu",
             "charac_sp", "charac_is", "charac_ft", "charac_imb_memory",
             "charac_imb_CPU", "charac_alya"),
    interface = c("eth", "ib", "ipoib")),
    replications=1,
    randomize=TRUE)

cmp223_charac %>%
  mutate(number=1:n()) -> cmp223_charac
write_csv(cmp223_charac, "R/experimental_project_charac.csv")
#+end_src

#+RESULTS:
: 
: creating full factorial with 33 runs ...

** Bash Scripts Descriptions
- [[SH/experiments_exec.sh][Experiments Execution]] - This is the most essential script for this
  evaluation. It comprises from the beggining, when the variables are
  defined, until the end, when the experiments execution ends. To
  describe it, it has been divided into a few steps, which are:

  - *Step 1*: Define the variables and Create the Folders - Here all the
    variables with folder, software and bencharmark locations are
    defined and created.
  - *Step 2*: Collect System Information - In this step, the Execution
    Experiments script calls the System Information script to collect
    information about all nodes used in the evaluation.
  - *Step 3*: Download and Compile the Programs - Here all the softwares
    and benchmarks are donwloaded and compiled with their respective
    compilers.
  - *Step 4*: Define Machine Files and Experimental Project - In this
    step the machine files and experimental project used during MPI
    execution are defined.
  - *Step 5*: Read the Experimental Project and Start the Execution
    Loop - This step reads the experimental project, start the MPI
    command line, executes the experiments with their respective
    interconnection and binaries. At the end of each execution, the
    results are sent to the their respective log files.
  - *Step 6*: Call the Experiment Characterization Script - This final
    step calls the experiment characterization script to start the
    characterization execution. Characterization and "normal"
    execution are done one after another because in characterization
    execution there is the trace process that records all the MPI
    primitives in a file and therefore can interfer in the normal
    execution considering resources usage (e.g., IO) at the end of
    execution.
- [[SH/experiments_charac.sh][Experiments Characterization]] - 
- [[SH/central.sh][Central]] - This script was created to allocate the nodes using the
  ~salloc~ command from Slurm Workload Manager and then pass the bash
  script execution command through ssh to start the Experiments
  Execution.
- [[SH/software_install.sh][Software Installation]] - This script is basically a simple loop to
  check whether packages within the 'name' vector are installed or
  not. If so, them ok. Otherwise, install the packages that are not
  installed. This script assume that the user has sudo
  privilegies. Otherwise, go to the *Software Installation* session,
  which will describe how to install the software using Spack packet
  manager.
- [[SH/sys_info_collect.sh][System Information Collect]] - This is a crucial script to performance
  evaluations, which executes before the benchmarks. It saves all the
  system information and sends the output to an ORG file. This output
  will undoubtedly help describe the results or even add system
  information to the paper/report.

** Experiments Execution
The scripts in this work are designed for use in a cluster with Slurm
job scheduler. Here, to start the experiments, the [[SH/central.sh][central script]] was
first executed, which allocates the necessary nodes, in this case,
hype2, hype3, hype4, and hype5, and passes through ssh the bash
command that calls the [[SH//experiments_exec.sh][experiments execution]] script. This script first
calls the [[SH/sys_info_collect.sh][system information]] script to collect system
information. Next, it executes the experiments and, at the end,
requests that the [[SH/experiments_charac.sh][experiments characterization]] script to start the
characterization execution.

To reproduce this evaluation in an environment without Slurm job
scheduler, simply clone this repository into the server's HOME
directory, set the server names in the PARTITION variable ([[SH/experiments_exec.sh][experiments
execution]] script line 66), adjust the machine files in [[MACHINE_FILES][Machine Files]]
folder also with the name of the servers, adjust the number of
processes (in [[SH/experiments_exec.sh][experiments execution]] script lines 237, 240, 243 and,
246 and in [[SH/experiments_charac.sh][experiments characterization]] script lines 93, 96 and, 99)
to be used during the execution, respecting the power of two or square
root requirements, and finally executes the [[SH/experiments_exec.sh][experiments execution]]
script like a normal bash script.

** Graphical Analysis 
After the conclusion of the experiments, in this topic, graphs
containing the execution time and the characterization of the
applications were created. The first step is to read the CSV
file. 
Next, four graphs are created according to the number of
processes, in which BT and SP have 121, and according to a similar
execution time range.

*** Read CSVsVVVss
#+begin_src R :results output :session *R* :exports both
options(crayon.enabled=FALSE)
library("tidyverse")
df_intel <- read_csv("/home/anderson/Desktop/LOGS/intel.21-11-2019.14h59m33s.csv")
df_npb <- read_csv("LOGS/npb.02-11-2019.21h37m51s.csv")
 df_npb$apps=toupper(df_npb$apps) 
    
    df_npb %>%
      group_by(apps,interface) %>%
      summarise(
        mean=mean(time),
        sd=sd(time),
        se=sd/sqrt(n()),
        N=n()) %>%
      arrange(apps,interface) -> df_npb
df_npb

df_intel %>%
  filter(bytes != 0) %>%
  group_by(interface,bytes) %>%
  summarise(
    average=mean(time),
    std=sd(time),
    ste=std/sqrt(n()),
    N=n()) %>%
  arrange(interface,bytes) -> df_intel_latency
df_intel_latency

df_intel %>%
  filter(bytes != 0) %>%
  group_by(interface,bytes) %>%
  summarise(
    average=mean(`mbytes-sec`),
    std=sd(`mbytes-sec`),
    ste=std/sqrt(n()),
    N=n()) %>%
  arrange(interface,bytes) -> df_intel_band
df_intel_band

#+end_src

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  apps = col_character(),
  interface = col_character(),
  bytes = col_double(),
  time = col_double(),
  `mbytes-sec` = col_double()
)

Error: 'LOGS/npb.02-11-2019.21h37m51s.csv' does not exist in current working directory ('/home/anderson/Desktop/GIT/CMP223').

Error in toupper(df_npb$apps) : object 'df_npb' not found

Error in eval(lhs, parent, parent) : object 'df_npb' not found

Error: object 'df_npb' not found

# A tibble: 69 x 6
# Groups:   interface [3]
   interface bytes average    std    ste     N
   <
    <
  <
 <
 <
<int>
 1 eth           1    7.60 2.18   1.26       3
 2 eth           2    7.34 2.33   1.35       3
 3 eth           4    6.71 1.22   0.704      3
 4 eth           8    5.82 0.173  0.1000     3
 5 eth          16    5.85 0.123  0.0709     3
 6 eth          32    5.77 0.100  0.0578     3
 7 eth          64    5.79 0.0737 0.0426     3
 8 eth         128    5.91 0.0764 0.0441     3
 9 eth         256    5.94 0.142  0.0821     3
10 eth         512    5.98 0.0961 0.0555     3
# â¦ with 59 more rows

# A tibble: 69 x 6
# Groups:   interface [3]
   interface bytes average    std    ste     N
   <
    <
  <
 <
 <
<int>
 1 eth           1   0.137 0.0321 0.0186     3
 2 eth           2   0.290 0.0794 0.0458     3
 3 eth           4   0.613 0.103  0.0593     3
 4 eth           8   1.38  0.0404 0.0233     3
 5 eth          16   2.73  0.0586 0.0338     3
 6 eth          32   5.55  0.0954 0.0551     3
 7 eth          64  11.0   0.147  0.0851     3
 8 eth         128  21.7   0.274  0.158      3
 9 eth         256  43.1   1.01   0.585      3
10 eth         512  85.7   1.37   0.791      3
# â¦ with 59 more rows
#+end_example

*** PingPong - Latency
#+begin_src R :results output graphics :file R/PLOTS/PingPong.png :exports both :width 600 :height 400 :session *R* 
ggplot(df_intel_latency,aes(x=bytes, y=average)) +
  geom_line(aes(col = interface), alpha = 1) +
  geom_point(aes(col = interface), size = 2.5) +
  geom_errorbar(aes(ymin=average-ste, ymax=average+ste, color=interface, group=interface), width = .25) +
  theme_bw() +
  scale_y_log10(breaks=c(0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024)) +
  scale_x_log10(breaks=c(0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304)) +
  ylab('Average Latency Time\n(Microseconds in Logscale)') +
  xlab('Message Size (Bytes)') +
  scale_colour_manual(values=c("#808080", "#000000", "#4F4F4F"), name="Network Interface",
                    breaks=c("ib", "ipoib", "eth"),
                    labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +
  theme (legend.position = c(0.2, 0.8),
         legend.background = element_rect(color = "black", size = 0.3, linetype = "solid"),
         axis.title=element_text(size=16), 
         legend.title = element_text(color = "black", size = 16),
         legend.text = element_text(color = "black", size = 16),
         axis.text.x = element_text(angle=55, hjust=1, size =14, color = "black"),
         axis.text.y = element_text(size =14, color = "black"))
#+end_src

#+RESULTS:
[[file:R/PLOTS/PingPong.png]]

*** PingPong - Bandwidth
#+begin_src R :results output graphics :file R/PLOTS/Bandwidth.png :exports both :width 600 :height 400 :session *R* 
ggplot(df_intel_band,aes(x=bytes, y=average)) +
  geom_line(aes(col = interface), alpha = 1) +
  geom_point(aes(col = interface), size = 2.5) +
  geom_errorbar(aes(ymin=average-ste, ymax=average+ste, color=interface, group=interface), width = .25) +
  theme_bw() +
  scale_y_log10(breaks=c(0.1, 1, 10, 100, 1000, 10000)) +
  scale_x_log10(breaks=c(0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304)) +
  ylab('Average Bandwidth\n(Megabytes per Second in Logscale)') +
  xlab('Message Size (Bytes)') +
  scale_colour_manual(values=c("#808080", "#000000", "#4F4F4F"), name="Network Interface",
                    breaks=c("ib", "ipoib", "eth"),
                    labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +
  theme (legend.position = c(0.2, 0.8),
         legend.background = element_rect(color = "black", size = 0.3, linetype = "solid"),
         axis.title=element_text(size=16), 
         legend.title = element_text(color = "black", size = 16),
         legend.text = element_text(color = "black", size = 16),
         axis.text.x = element_text(angle=55, hjust=1, size =14, color = "black"),
         axis.text.y = element_text(size =14, color = "black"))
#+end_src

#+RESULTS:
[[file:R/PLOTS/Bandwidth.png]]

*** BT-SP(121 Procs)
#+begin_src R :results output graphics :file PLOTS/BT_SP.png :exports both :width 600 :height 400 :session *R*
ggplot(newdf[newdf$apps %in% c("BT", "SP"), ] , aes(x=apps, y=mean, fill=interface)) +
    geom_bar(stat="identity", position = "dodge", width = 0.2) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.05, position = position_dodge(.2)) +
    theme_minimal() +
    scale_fill_manual(values=c("#006dff", "#5ca3ff", "#b2d3ff"), name="Network\nInterface",
    breaks=c("ETH", "IB", "IPoIB"), labels=c("Ethernet", "InfiniBand", "IP-over-IB")) +
    theme(legend.position = c(0.15, 0.85), legend.background = element_rect(color = "black",
    size = 0.3, linetype = "solid"), axis.text=element_text(size=12), 
    axis.title=element_text(size=12), legend.title = element_text(color = "black", size = 14),
    legend.text = element_text(color = "black", size = 12)) +
    labs(x="Application", y="Execution Time [s]")
#+end_src

#+RESULTS:
[[file:PLOTS/BT_SP.png]]

*** FT(128 Procs)
#+begin_src R :results output graphics :file PLOTS/FT.png :exports both :width 600 :height 400 :session *R* 
   ggplot(newdf[newdf$apps %in% c("FT"), ] , aes(x=apps, y=mean, fill=interface)) +
    geom_bar(stat="identity", position = "dodge", width = 0.1) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.05, position = position_dodge(.1)) +
    theme_minimal() +
    scale_fill_manual(values=c("#006dff", "#5ca3ff", "#b2d3ff"), name="Network\nInterface",
    breaks=c("ETH", "IB", "IPoIB"), labels=c("Ethernet", "InfiniBand", "IP-over-IB")) +
    theme(legend.position = c(0.15, 0.85), legend.background = element_rect(color = "black",
    size = 0.3, linetype = "solid"), axis.text=element_text(size=12), 
    axis.title=element_text(size=12), legend.title = element_text(color = "black", size = 14),
    legend.text = element_text(color = "black", size = 12)) +
    labs(x="Application", y="Execution Time [s]")
#+end_src

#+RESULTS:
[[file:PLOTS/FT.png]]

*** EP-IS-MG(128 Procs)
#+begin_src R :results output graphics :file PLOTS/EP_IS_MG.png :exports both :width 600 :height 400 :session *R* 
ggplot(newdf[newdf$apps %in% c("EP", "IS", "MG"), ] , aes(x=apps, y=mean, fill=interface)) +
    geom_bar(stat="identity", position = "dodge", width = 0.3) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.1, position = position_dodge(.3)) +
    theme_minimal() +
    scale_fill_manual(values=c("#006dff", "#5ca3ff", "#b2d3ff"), name="Network\nInterface",
    breaks=c("ETH", "IB", "IPoIB"), labels=c("Ethernet", "InfiniBand", "IP-over-IB")) +
    theme(legend.position = c(0.15, 0.85), legend.background = element_rect(color = "black",
    size = 0.3, linetype = "solid"), axis.text=element_text(size=12), 
    axis.title=element_text(size=12), legend.title = element_text(color = "black", size = 14),
    legend.text = element_text(color = "black", size = 12)) +
    labs(x="Application", y="Execution Time [s]")
#+end_src

#+RESULTS:
[[file:PLOTS/EP_IS_MG.png]]
*** CG-LU(128 Procs)
#+begin_src R :results output graphics :file PLOTS/CG_LU.png :exports both :width 600 :height 400 :session *R* 
ggplot(newdf[newdf$apps %in% c("CG", "LU"), ] , aes(x=apps, y=mean, fill=interface)) +
    geom_bar(stat="identity", position = "dodge", width = 0.2) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.05, position = position_dodge(.2)) +
    theme_minimal() +
    scale_fill_manual(values=c("#006dff", "#5ca3ff", "#b2d3ff"), name="Network\nInterface",
    breaks=c("ETH", "IB", "IPoIB"), labels=c("Ethernet", "InfiniBand", "IP-over-IB")) +
    theme(legend.position = c(0.9, 0.85), legend.background = element_rect(color = "black",
    size = 0.3, linetype = "solid"), axis.text=element_text(size=12), 
    axis.title=element_text(size=12), legend.title = element_text(color = "black", size = 14),
    legend.text = element_text(color = "black", size = 12)) +
    labs(x="Application", y="Execution Time [s]")
#+end_src

#+RESULTS:
[[file:PLOTS/CG_LU.png]]

