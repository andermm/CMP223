#+TITLE: LabBook
#+AUTHOR: Anderson Mattheus Maliszewski
#+STARTUP: overview indent
#+TAGS: noexport(n) deprecated(d) 
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

This LabBook is for the *CMP223 - Computer System Performance* *Analysis
(2019/2)* final work and has a wealth of informations about the entire
process used during the proposed experiments. To reproduce this
evaluation, you can follow the steps below. Have fun!

#+BEGIN_QUOTE
*Reporting errors*: This repository has several links to self-contained
 files as well as links from the Internet, so if you try to reproduce
 it and find broken links or other problems, please tell me everything
 so that I can improve it. :)
#+END_QUOTE

* Experimental Project
** Objective
   Assess the impact of the network interconnection on HPC
   applications. Both *InfiniBand (IB)* and *Gigabit Ethernet (ETH)*, as
   well as *IP-over-IB (IPoIB)* interconnections were evaluated using
   the same physical cluster of servers. The MPI-PingPong benchmark of
   the [[https://software.intel.com/en-us/articles/intel-mpi-benchmarks][Intel MPI Benchmarks]] suite was executed to first characterize
   the interconnection latency and throughput. Then two synthetic
   benchmarks (NPB and ImbBench) and one real application (Alya) were
   executed. [[https://www.nas.nasa.gov/publications/npb.html][NAS Parallel Benchmarks (NPB)]] set version 3.4 with input
   *class D* was used because it represent several patterns of real HPC
   applications, as well as being characterized as a balanced
   benchmarks set (when an MPI parallel application distributes work
   between processes evenly, and therefore all end almost together,
   making the best use of available resources). On the other hand,
   [[https://github.com/Roloff/ImbBench.git][Inbalanced Benchmark (IMB)]] was created to represent the more common
   inbalanced application patterns. Thus, with both of the mentioned
   benchmarks we cover the common HPC application and also the
   balanced and inbalanced patterns. [[https://www.bsc.es/research-development/research-areas/engineering-simulations/alya-high-performance-computational][Alya]] was used because this work
   tends to cover not only synthetic benchmarks which try to mimic
   real applications but rather use a real application.

** Benchmarks
Below is a brief overview about the benchmarks used.
*** Intel MPI Benchmark
The Intel® MPI Benchmarks perform a set of MPI performance
measurements for point-to-point and global communication operations
for a range of message sizes. The generated benchmark data fully
characterizes:
- Performance of a cluster system, including node performance, network
  latency, and throughput.
- Efficiency of the MPI implementation used.
There are several benchmarks included in this set, and in this
evaluation, only the MPI1 PingPong application was used to measure
interconnect latency and throughput.

*** NAS Parallel Benchmarks
The NAS Parallel Benchmarks (NPB) are a small set of programs designed
to help evaluate the performance of parallel supercomputers. The
benchmarks are derived from computational fluid dynamics (CFD)
applications. Problem sizes in NPB are predefined and indicated as
different classes.

In this evaluation, the original set of benchmarks from the NPB suite,
consisting of five kernels and three pseudo-applications were used
with the Messsage Passing Interface (MPI) parallel implementation.

Five Kernels:
- *IS* - Integer Sort, random memory access.
- *EP* - Embarrassingly Parallel.
- *CG* - Conjugate Gradient, irregular memory access and communication.
- *FT* - Discrete 3D fast Fourier Transform, all-to-all communication.
- *MG* - Multi-Grid on a sequence of meshes, long- and short-distance
  communication, memory intensive.

Three pseudo-applications: 
- *BT* - Block Tri-diagonal solver.
- *SP* - Scalar Penta-diagonal solver.
- *LU* - Lower-Upper Gauss-Seidel solver.

They were executed with 128 processes on 4 nodes, 32 processes per
node, in the case of IS, EP, CG, FT, MG, and LU (power-of-two). Since
BT and SP require that the number of processes to be a square root,
144 processes were used, with 36 processes in each node.

*** Inbalanced Benchmark
Imbalance Benchmark (ImbBench) is a set of MPI-based applications,
created by the Ph.D. Student Eduardo Roloff, that simulate several
behaviors in terms of process loads. ImbBench was designed with the
cloud heterogeneity in mind, and its goal is to help the user to
choose the most suitable configuration to execute an application in
the cloud. ImbBench distributes the load among all the available
processes according to a preselected imbalanced pattern.

ImbBench has a set of microbenchmarks and parameters to benchmark CPU
and Memory. In this evaluation, it was used both CPU and Memory
benchmarks with the 8Level pattern of imbalance and the Rand and BST
microbenchmarks, respectively.

*** Alya
Alya is a high performance computational mechanics code to solve
complex coupled multi-physics / multi-scale / multi-domain problems,
which are mostly coming from the engineering realm. Among the
different physics solved by Alya we can mention:
incompressible/compressible flows, non-linear solid mechanics,
chemistry, particle transport, heat transfer, turbulence modeling,
electrical propagation, etc.

** How to Reproduce it
To reproduce this project, the first step is to to clone the git repository in
the *$HOME* of the desired cluster of servers. 

#+begin_src shell :results output :exports both
cd $HOME; git clone https://github.com/andermm/CMP223
#+end_src

Here it is assumed that your HOME directory is exported with NFS. If
you don't have the NFS configured, [[https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04][here]] is a tutorial to do it. If
this step is ok, go ahead to the other topics.

** Required Softwares
To execute the experiments, some required packages need to
be installed. They are:
- openmpi-bin - High performance message passing library (mpiexec).
- libopenmpi-dev - High performance message passing library (mpicc and
  mpifort).
- gfortran - GNU Fortran 95 compiler.
- cmake - Cross-platform, open-source make system.
- pajeng - Space-time view and associated tools for Paje trace files.

If your OS is Debian based and you have permission to install new
packages, you can install the required packages using the [[SH/software_install.sh][Software
Installer]] script. On the other hand, if you do not have permission to
do so, you can use the Spack package manager. The full tutorial on how
to use Spack can be seen [[https://spack-tutorial.readthedocs.io/en/latest/][here]].

** System Information 
   To collect the information of all nodes used in the evaluation, it
   was used the [[SH/sys_info_collect.sh][System Information Collect]] script, which creates the
   [[LOGS/env_info.org][System Information]] log output with ORG extension. The script
   executes automatically in the execution script.

** Network Infrastructure Information
Each node has a Mellanox MT27600 Channel Adapter (CA) configured for
the InfiniBand 56 Gb/s 4X FDR ConnectX-3 with firmware version
10.16.1038 and OFED version 4.6-1.0.1.1. All nodes are interconnected
through a Mellanox SX6036 FDR and a generic de 1 Gbps switch.

** Design of Experiments
   To execute benchmarks without bias, the DoE.base library was used
   to create Experiment Design (DoE). In DoE, two factors were used,
   applications and interface, with 30 random replications, totaling
   1080 (12 * 3 * 30) distinct executions. For the characterization of
   the applications, one random replication was performed totaling 33
   (11 * 3) distinct executions (the MPI PingPong benchmark was not
   performed in the characterization step).

Execution factors:
- Factor 1 - Apps: The applications name totaling 12 (~exec_bt~,
  ~exec_ep~, ~exec_cg~, ~exec_mg~, ~exec_lu~, ~exec_sp~, ~exec_is~, ~exec_ft~,
  ~exec_imb_memory~, ~exec_imb_CPU~, ~exec_intel~, and ~exec_alya~).
- Factor 2 - Interface: The network interface names, totaling 3 (~eth~,
  ~ib~, ~ipoib~).

Characterization factors:
- Factor 1 - Apps: The applications name totaling 11 (~charac_bt~,
  ~charac_ep~, ~charac_cg~, ~charac_mg~, ~charac_lu~, ~charac_sp~, ~charac_is~,
  ~charac_ft~, ~charac_imb_memory~, ~charac_imb_CPU~, and ~charac_alya~).
- Factor 2 - Interface: The network interface names, totaling 3 (~eth~,
  ~ib~, ~ipoib~).

Below is the R code block which generates the CSV files.
#+begin_src R :results output :session *R* :exports both
options(crayon.enabled=FALSE)
library(DoE.base)
library(tidyverse)

set.seed(0)
  cmp223_exec <- fac.design(factor.names = list(
    apps = c("exec_bt", "exec_ep", "exec_cg", "exec_mg", "exec_lu",
             "exec_sp", "exec_is", "exec_ft", "exec_imb_memory",
             "exec_imb_CPU", "exec_intel", "exec_alya"),
    interface = c("eth", "ib", "ipoib")),
    replications=30,
    randomize=TRUE)

cmp223_exec %>%
  select(-Blocks) %>%
  mutate(number=1:n()) -> cmp223_exec
write_csv(cmp223_exec, "R/experimental_project_exec.csv")
#+end_src

#+RESULTS:
: 
: creating full factorial with 36 runs ...

#+begin_src R :results output :session *R* :exports both
options(crayon.enabled=FALSE)
library(DoE.base)
library(tidyverse)

set.seed(0)
  cmp223_charac <- fac.design(factor.names = list(
    apps = c("charac_bt", "charac_ep", "charac_cg", "charac_mg", "charac_lu",
             "charac_sp", "charac_is", "charac_ft", "charac_imb_memory",
             "charac_imb_CPU", "charac_alya"),
    interface = c("eth", "ib", "ipoib")),
    replications=1,
    randomize=TRUE)

cmp223_charac %>%
  mutate(number=1:n()) -> cmp223_charac
write_csv(cmp223_charac, "R/experimental_project_charac.csv")
#+end_src

#+RESULTS:
: 
: creating full factorial with 33 runs ...

** Bash Scripts Descriptions
- [[SH/experiments_exec.sh][Experiments Execution]] - This is the most essential script for this
  evaluation. It comprises from the beginning, when the variables are
  defined, until the end, when the experiments execution ends. To
  describe it, it has been divided into a few steps, which are:

  - *Step 1*: Define the variables and Create the Folders - Here all the
    variables with folders, software and bencharmark locations are
    defined and created.
  - *Step 2*: Collect System Information - In this step, the Execution
    Experiments script calls the System Information script to collect
    information about all nodes used in the evaluation.
  - *Step 3*: Download and Compile the Programs - Here all the softwares
    and benchmarks are downloaded and compiled with their respective
    compilers.
  - *Step 4*: Define Machine Files and Experimental Project - In this
    step the machine files and experimental project used during MPI
    execution are defined.
  - *Step 5*: Read the Experimental Project and Start the Execution
    Loop - This step reads the experimental project, start the MPI
    command line, and executes the experiments with their respective
    interconnection/application. At the end of each execution, the
    results are sent to the log files.
  - *Step 6*: Call the Experiment Characterization Script - This final
    step calls the experiment characterization script to start the
    characterization execution. Characterization and "normal"
    execution are done one after another because in characterization,
    the execution trace process records all the MPI primitives in a
    file and therefore can interfer in the normal execution
    considering resources usage (e.g., IO).
- [[SH/experiments_charac.sh][Experiments Characterization]] - This script is similar to the one
  discribed above, however, it only performs the experiment
  characterization, subdivided into three steps, which are:

  - *Step 1*: A first definition of the variables.
  - *Step 2*: Definition of the Machines Files and Experimental Project.
  - *Step 3*: Read the Experimental Project and Start the Execution Loop

- [[SH/central.sh][Central]] - This script was created to allocate the nodes using the
  ~salloc~ command from Slurm Workload Manager and then pass the bash
  script execution command through ssh to start the Experiments
  Execution.
- [[SH/software_install.sh][Software Installation]] - This script is basically a simple loop to
  check whether packages within the 'name' vector are installed or
  not. If so, them ok. Otherwise, install the packages that are not
  installed. This script assume that the user has sudo
  privilegies. Otherwise, go to the *Software Installation* session,
  which will describe how to install the software using Spack packet
  manager.
- [[SH/sys_info_collect.sh][System Information Collect]] - This is a crucial script to performance
  evaluations, which is executed before the benchmarks. It saves all the
  system information and sends the output to an ORG file. This output
  will undoubtedly help describe the results or even add system
  information to the paper/report.

** Experiments Execution
The scripts in this work are designed for use in a cluster with Slurm
job scheduler. Here, to start the experiments, the [[SH/central.sh][central script]] was
first executed, which allocates the necessary nodes, in this case,
hype2, hype3, hype4, and hype5, and passes through ssh the bash
command that calls the [[SH//experiments_exec.sh][experiments execution]] script. This script first
calls the [[SH/sys_info_collect.sh][system information]] script to collect system
information. Next, it executes the experiments and, at the end,
requests that the [[SH/experiments_charac.sh][experiments characterization]] script to start the
characterization execution.

To reproduce this evaluation in an environment without Slurm job
scheduler, simply clone this repository into the server's HOME
directory, set the server names in the PARTITION variable ([[SH/experiments_exec.sh][experiments
execution]] script line 66), adjust the machine files in [[MACHINE_FILES][Machine Files]]
folder also with the name of the servers, adjust the number of
processes (in [[SH/experiments_exec.sh][experiments execution]] script lines 237, 240, 243 and,
246 and in [[SH/experiments_charac.sh][experiments characterization]] script lines 93, 96 and, 99)
to be used during the execution, respecting the power of two or square
root requirements, and finally executes the [[SH/experiments_exec.sh][experiments execution]]
script like a normal bash script.

** Graphical Analysis 
Upon completion of the experiments, in this topic, graphs containing
the execution time and the characterization of the applications are
created. The first step is to read the CSV files. Latency and
bandwidth graphs are then created. Finally, applications graphs are
created according to the number of processes, in which BT and SP have
144 and according to a similar execution time range.

*** Read CSVs
#+begin_src R :results output :session *R* :exports both
options(crayon.enabled=FALSE)
suppressMessages(library("tidyverse"));
df_intel <- read_csv("LOGS/intel.22-11-2019.16h44m04s.csv")
df_apps <- read_csv("LOGS/apps_exec.22-11-2019.16h44m04s.csv")
    
df_apps %>%
      group_by(apps,interface) %>%
      summarise(
        average=mean(time),
        std=sd(time),
        ste=3*std/sqrt(n()),
        N=n()) %>%
      arrange(apps,interface) -> df_apps
df_apps

df_intel %>%
  filter(bytes != 0) %>%
  group_by(interface,bytes) %>%
  summarise(
    average=mean(time),
    std=sd(time),
    ste=3*std/sqrt(n()),
    N=n()) %>%
  arrange(interface,bytes) -> df_intel_latency
df_intel_latency

df_intel %>%
  filter(bytes != 0) %>%
  group_by(interface,bytes) %>%
  summarise(
    average=mean(`mbytes-sec`),
    std=sd(`mbytes-sec`),
    ste=3*std/sqrt(n()),
    N=n()) %>%
  arrange(interface,bytes) -> df_intel_band
df_intel_band

#+end_src

#+RESULTS:
#+begin_example

Parsed with column specification:
cols(
  apps = col_character(),
  interface = col_character(),
  bytes = col_double(),
  time = col_double(),
  `mbytes-sec` = col_double()
)

Parsed with column specification:
cols(
  apps = col_character(),
  interface = col_character(),
  time = col_double()
)

# A tibble: 33 x 6
# Groups:   apps [11]
   apps      interface average   std   ste     N
   <
    <
      <
<
<
<int>
 1 exec_alya eth         297.  1.77  0.968    30
 2 exec_alya ib          254.  2.79  1.53     30
 3 exec_alya ipoib       281.  2.90  1.59     30
 4 exec_bt   eth         505.  1.54  0.842    30
 5 exec_bt   ib          380.  4.17  2.28     30
 6 exec_bt   ipoib       544.  3.51  1.92     30
 7 exec_cg   eth         761.  8.24  4.51     30
 8 exec_cg   ib          183.  1.01  0.551    30
 9 exec_cg   ipoib       181.  2.58  1.41     30
10 exec_ep   eth          37.4 0.422 0.231    30
# … with 23 more rows

# A tibble: 69 x 6
# Groups:   interface [3]
   interface bytes average   std   ste     N
   <
    <
  <
<
<
<int>
 1 eth           1    6.42 0.456 0.250    30
 2 eth           2    6.28 0.470 0.257    30
 3 eth           4    6.18 0.472 0.258    30
 4 eth           8    6.07 0.381 0.209    30
 5 eth          16    6.02 0.338 0.185    30
 6 eth          32    5.98 0.301 0.165    30
 7 eth          64    5.98 0.280 0.153    30
 8 eth         128    5.99 0.272 0.149    30
 9 eth         256    5.99 0.247 0.136    30
10 eth         512    6.02 0.218 0.119    30
# … with 59 more rows

# A tibble: 69 x 6
# Groups:   interface [3]
   interface bytes average    std     ste     N
   <
    <
  <
 <
  <
<int>
 1 eth           1   0.157 0.0106 0.00578    30
 2 eth           2   0.320 0.0230 0.0126     30
 3 eth           4   0.651 0.0487 0.0267     30
 4 eth           8   1.32  0.0821 0.0450     30
 5 eth          16   2.67  0.147  0.0803     30
 6 eth          32   5.36  0.264  0.145      30
 7 eth          64  10.7   0.488  0.267      30
 8 eth         128  21.4   0.945  0.518      30
 9 eth         256  42.8   1.72   0.940      30
10 eth         512  85.2   2.98   1.63       30
# … with 59 more rows
#+end_example
*** Execution Time Plots
**** PingPong - Latency
#+begin_src R :results output graphics :file R/PLOTS/PingPong.png :exports both :width 800 :height 600 :session *R* 
ggplot(df_intel_latency,aes(x=bytes, y=average)) +
  geom_line(aes(color = interface), alpha = 1) +
  geom_point(aes(color=interface, shape=interface),size = 4) +
  scale_shape_manual(values = c(15, 16, 17),
                     breaks=c("ib", "ipoib", "eth"),
                     labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +
  geom_errorbar(aes(ymin=average-ste, ymax=average+ste, color=interface, group=interface), width = .2) +
  theme_bw() +
  scale_y_log10(breaks=c(0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024)) +
  scale_x_log10(breaks=c(0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304)) +
  ylab('Average Latency Time\n(Microseconds in 2 Log Scale)') +
  xlab('Message Size (Bytes)') +
  scale_color_manual(values=c( "#BEBEBE", "#303030", "#888888"),
                    breaks=c("ib", "ipoib", "eth"),
                    labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +
    theme (legend.position = c(0.15, 0.85),
           legend.background = element_rect(color = "black", size = 0.5, linetype = "solid"),
           plot.margin = unit(x = c(0, 0, 0, 0), units = "cm"),
           legend.margin=margin(c(0, 0, -8, 0)),
           axis.title=element_text(size=18), 
           legend.title = element_blank(),
           legend.text = element_text(color = "black", size = 18),
           axis.text.x = element_text(angle=55, hjust=1, size =16, color = "black"),
           axis.text.y = element_text(size =16, color = "black"))
#+end_src

#+RESULTS:
[[file:R/PLOTS/PingPong.png]]

**** PingPong - Bandwidth
#+begin_src R :results output graphics :file R/PLOTS/Bandwidth.png :exports both :width 800 :height 600 :session *R* 
ggplot(df_intel_band,aes(x=bytes, y=average)) +
  geom_line(aes(col = interface), alpha = 1) +
  geom_point(aes(col = interface, shape=interface), size = 4) +
  scale_shape_manual(values = c(15, 16, 17),
                     breaks=c("ib", "ipoib", "eth"),
                     labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +  
  geom_errorbar(aes(ymin=average-ste, ymax=average+ste, color=interface, group=interface), width = .2) +
  theme_bw() +
  scale_y_continuous(breaks=c(0,1000,2000,3000,4000,5000,6000,7000,8000)) +
  scale_x_log10(breaks=c(0, 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, 524288, 1048576, 2097152, 4194304)) +
  ylab('Average Bandwidth\n(Megabytes per Second)') +
  xlab('Message Size (Bytes)') +
  scale_colour_manual(values=c( "#BEBEBE", "#303030", "#888888"),
                    breaks=c("ib", "ipoib", "eth"),
                    labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +
  theme (legend.position = c(0.15, 0.85),
         legend.background = element_rect(color = "black", size = 0.3, linetype = "solid"),
          plot.margin = unit(x = c(0, 0, 0, 0), units = "cm"),
         legend.margin=margin(c(0, 0, -8, 0)),
         axis.title=element_text(size=18), 
         legend.title = element_blank(),
         legend.text = element_text(color = "black", size = 18),
         axis.text.x = element_text(angle=55, hjust=1, size =16, color = "black"),
         axis.text.y = element_text(size =16, color = "black"))
#+end_src

#+RESULTS:
[[file:R/PLOTS/Bandwidth.png]]

**** FT-EP-IS-SP
#+begin_src R :results output graphics :file R/PLOTS/FT_EP_IS_SP.png :exports both :width 800 :height 400 :session *R*
try <- c(exec_ft="FT", exec_sp="SP",  exec_ep="EP", exec_is="IS")
try2 <- c(exec_ft="128", exec_sp="144",  exec_ep="128", exec_is="128")
df_apps$interface <- factor(df_apps$interface,
levels=c("ib", "ipoib", "eth"))

ggplot(df_apps[df_apps$apps %in% c("exec_ft", "exec_sp", "exec_ep", "exec_is"), ] , aes(x=reorder(apps, +average), y=average, fill=interface)) +
geom_bar(stat="identity", position = "dodge", width = 0.6) +
geom_errorbar(aes(ymin=average-ste, ymax=average+ste), width=0.2, position = position_dodge(.6)) +
theme_bw() +
scale_fill_manual(values=c("#303030", "#888888", "#BEBEBE") ,
breaks=c("ib", "ipoib", "eth"), labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +
theme(legend.position = "top", 
      legend.key = element_rect(colour = "grey"),
      legend.key.height = unit(0.5, "line"),
      legend.key.width = unit(4, "line"),
      legend.spacing = unit(100, "line"),
      plot.margin = unit(x = c(0, 0, 0, 0), units = "cm"),
      legend.margin=margin(c(0, 0, -8, 0)),
      axis.text.x = element_text(size =16, color = "black"),
      axis.text.y = element_text(size =16, color = "black"),
      axis.title=element_text(size=18), 
      legend.title = element_blank(),
      legend.text = element_text(color = "black", size = 18)) +
      scale_x_discrete(labels=try2) +
      labs(x="Number of MPI Processes", y="Execution Time in Seconds") +
      facet_wrap(facets= ~ apps, scales="free", labeller=labeller(apps=try), nrow=1) +
      theme(strip.text.x=element_text(size=18),
            strip.background=element_rect(fill='#F5F5F5'))
     
#+end_src

#+RESULTS:
[[file:R/PLOTS/FT_EP_IS_SP.png]]

**** MG-CG-LU-BT
#+begin_src R :results output graphics :file R/PLOTS/MG_CG_LU_BT.png :exports both :width 800 :height 400 :session *R*
try <- c(exec_mg="MG", exec_cg="CG",  exec_lu="LU", exec_bt="BT")
try2 <- c(exec_mg="128", exec_cg="128",  exec_lu="128", exec_bt="144")
df_apps$interface <- factor(df_apps$interface,
levels=c("ib", "ipoib", "eth"))

ggplot(df_apps[df_apps$apps %in% c("exec_mg", "exec_cg", "exec_lu", "exec_bt"), ] , aes(x=reorder(apps, +average), y=average, fill=interface)) +
geom_bar(stat="identity", position = "dodge", width = 0.6) +
geom_errorbar(aes(ymin=average-ste, ymax=average+ste), width=0.2, position = position_dodge(.6)) +
theme_bw() +
scale_fill_manual(values=c("#303030", "#888888", "#BEBEBE") ,
breaks=c("ib", "ipoib", "eth"), labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +
theme(legend.position = "top", 
      legend.key = element_rect(colour = "grey"),
      legend.key.height = unit(0.5, "line"),
      legend.key.width = unit(4, "line"),
      legend.spacing = unit(100, "line"),
      plot.margin = unit(x = c(0, 0, 0, 0), units = "cm"),
      legend.margin=margin(c(0, 0, -8, 0)),
      axis.text.x = element_text(size =16, color = "black"),
      axis.text.y = element_text(size =16, color = "black"),
      axis.title=element_text(size=18), 
      legend.title = element_blank(),
      legend.text = element_text(color = "black", size = 18)) +
      scale_x_discrete(labels=try2) +
      labs(x="Number of MPI Processes", y="Execution Time in Seconds") +
      facet_wrap(facets= ~ apps, scales="free", labeller=labeller(apps=try), nrow=1) +
      theme(strip.text.x=element_text(size=18),
            strip.background=element_rect(fill='#F5F5F5'))
#+end_src

#+RESULTS:
[[file:R/PLOTS/MG_CG_LU_BT.png]]

**** Alya-IMB(160 Procs)
#+begin_src R :results output graphics :file R/PLOTS/Alya-IMB.png :exports both :width 800 :height 400 :session *R*  
try <- c(exec_alya="Alya",exec_imb_memory="ImbBench-Memory\n8Level", exec_imb_CPU="ImbBench-CPU\n8Level")
try2 <- c(exec_alya="160", exec_imb_memory="160", exec_imb_CPU="160")
df_apps$interface <- factor(df_apps$interface,
levels=c("ib", "ipoib", "eth"))

ggplot(df_apps[df_apps$apps %in% c("exec_alya", "exec_imb_memory", "exec_imb_CPU"), ] , aes(x=apps, y=average, fill=interface)) +
geom_bar(stat="identity", position = "dodge", width = 0.4) +
geom_errorbar(aes(ymin=average-ste, ymax=average+ste), width=.1, position = position_dodge(.4)) +
theme_bw() +
scale_fill_manual(values=c("#303030", "#888888", "#BEBEBE"),
breaks=c("ib", "ipoib", "eth"), labels=c("InfiniBand", "IP-over-IB", "Ethernet")) +
theme(legend.position = "top", 
      legend.key = element_rect(colour = "grey"),
      legend.key.height = unit(0.5, "line"),
      legend.key.width = unit(4, "line"),
      legend.spacing = unit(100, "line"),
      plot.margin = unit(x = c(0, 0, 0, 0), units = "cm"),
      legend.margin=margin(c(0, 0, -8, 0)),
      axis.text.x = element_text(size =16, color = "black"),
      axis.text.y = element_text(size =16, color = "black"),
      axis.title=element_text(size=18), 
      legend.title = element_blank(),
      legend.text = element_text(color = "black", size = 18)) +
      scale_x_discrete(labels=try2) +
      labs(x="Number of MPI Processes", y="Execution Time in Seconds") +
      facet_wrap(facets= ~ apps, scales="free", labeller=labeller(apps=try), nrow=1) +
      theme(strip.text.x=element_text(size=18),
            strip.background=element_rect(fill='#F5F5F5'))



#+end_src

#+RESULTS:
[[file:R/PLOTS/Alya-IMB.png]]
*** Characterization Plots
**** Alya-IMB
#+begin_src R :results output graphics :file R/PLOTS/Alya-IMB.charac.png :exports both :width 800 :height 400 :session *R* 
options(crayon.enabled=FALSE)
suppressMessages(library(tidyverse))
df.imb.CPU <- read_csv("LOGS/TRACE/charac_imb_CPU.eth/charac_imb_CPU.eth.csv", col_names = FALSE) %>%
  mutate(Rank = gsub("MPI Rank", "", X2)) %>%
  select(-X1, -X2, -X3, -X7) %>%
  rename(Start = X4,
         End = X5,
         Duration = X6,
         Value = X8) %>%
  mutate(Rank = as.integer(Rank)) %>%
  mutate(App = "ImbBench-CPU\n8Level") %>%
  select(Rank, everything())
df.imb.CPU

df.imb.mem <- read_csv("LOGS/TRACE/charac_imb_memory.eth/charac_imb_memory.eth.csv", col_names = FALSE) %>%
  mutate(Rank = gsub("MPI Rank", "", X2)) %>%
  select(-X1, -X2, -X3, -X7) %>%
  rename(Start = X4,
         End = X5,
         Duration = X6,
         Value = X8) %>%
  mutate(Rank = as.integer(Rank)) %>%
  mutate(App = "ImbBench-Memory\n8Level") %>%
  select(Rank, everything())
df.imb.mem

df.alya <- read_csv("LOGS/TRACE/charac_alya.eth/charac_alya.eth.csv", col_names = FALSE) %>%
  mutate(Rank = gsub("MPI Rank", "", X2)) %>%
  select(-X1, -X2, -X3, -X7) %>%
  rename(Start = X4,
         End = X5,
         Duration = X6,
         Value = X8) %>%
  mutate(Rank = as.integer(Rank)) %>%
  mutate(App = "Alya") %>%
  select(Rank, everything())
df.alya

df <- rbind(df.imb.CPU, df.imb.mem, df.alya)

df %>%
  filter(Value != "MPI_Finalize") %>%
  group_by(Rank,App) %>%
  summarize(MPI.Time = sum(Duration),
            Full.Time = max(End) - min(Start)) %>%
  mutate(Compute.Time = Full.Time - MPI.Time) %>%
  mutate(Comm.Ratio = MPI.Time / Full.Time * 100) %>%
  arrange(Rank) -> df.timings;
df.timings

df %>%
  group_by(Rank,App) %>%
  summarize(Time = max(End) - min(Start) - sum(Duration)) %>%
  mutate(Value = "Computing") -> df.compute
df.compute

df %>%
  filter(Value != "MPI_Finalize") %>%
  group_by(Rank,Value,App) %>%
  summarize(Time = sum(Duration)) -> df.communication;

df.communication$Value <- as.character(df.communication$Value)
df.communication$Value [grepl('MPI_*', df.communication$Value)] <- 'MPI'

df.communication %>%
  group_by(Rank, Value,App) %>%
  summarize(Time = sum(Time)) -> df.communication;


df.compute %>%
  bind_rows(df.communication) %>%
  ggplot(aes(x = Rank, y = Time, fill=Value)) +
  geom_bar(stat='identity', width=1) +
  scale_fill_manual(values=c("#303030", "#BEBEBE")) +
  scale_x_continuous(breaks=c(0,40,80,120,159)) +
  theme_bw() +
  theme(legend.position="top",
        legend.key = element_rect(colour = "grey"),
        legend.key.height = unit(0.5, "line"),
        legend.key.width = unit(4, "line"),
        legend.spacing = unit(100, "line"),
        plot.margin = unit(x = c(0, 0.2, 0, 0), units = "cm"),
        legend.margin=margin(c(0, 0, -8, 0)),
        axis.text.x = element_text(size =16, color = "black"),
        axis.text.y = element_text(size =16, color = "black"),
        axis.title=element_text(size=18), 
        legend.title = element_blank(),
        legend.text = element_text(color = "black", size = 18)) +
        labs(x="Number of MPI Ranks", y="Execution Time in Seconds") +
        facet_wrap(facets= ~ App, scales="free") +
        theme(strip.text.x=element_text(size=18),
        strip.background=element_rect(fill='#F5F5F5'))

#+end_src
