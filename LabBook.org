#+TITLE: LabBook
#+AUTHOR: Anderson Mattheus Maliszewski
#+STARTUP: overview indent
#+TAGS: noexport(n) deprecated(d) 
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

This LabBook is for the *Scientific Methodology and Performance
Evaluation for Experimental Computer* class and has the step-by-step
procedure used during the proposed experiments. Also, to reproduce
this evaluation, you can follow the steps below. Have fun!

* Experimental Project
** Objective
   Assess the impact of the network interconnection on HPC
   applications. Both *InfiniBand (IB)* and *Gigabit Ethernet (ETH)*, as
   well as *IP-over-IB (IPoIB)* interconnections were evaluated using
   the same physical cluster of servers. The MPI-PingPong benchmark of
   the [[https://software.intel.com/en-us/articles/intel-mpi-benchmarks][Intel MPI Benchmarks]] suite was executed to first characterize
   interconnect latency and throughput. Then two synthetic benchmarks
   (NPB and ImbBench) and two real applications (Alya and Ondes3d)
   were executed. [[https://www.nas.nasa.gov/publications/npb.html][NAS Parallel Benchmarks (NPB)]] set version 3.4 with
   input *class D* was used because it represent several patterns of
   real HPC applications, as well as being characterized as a balanced
   benchmarks sed (when and MPI parallel application makes balanced
   use of processes and consequently they all end almost together). On
   the other hand, [[https://github.com/Roloff/ImbBench.git][Inbalanced Benchmark (IMB)]] was created to represent
   the more common inbalanced application patterns. Thus, with both of
   the mentioned benchmarks we cover the common HPC application and
   also the balanced and unbalanced patterns. [[https://www.bsc.es/research-development/research-areas/engineering-simulations/alya-high-performance-computational][Alya]] and [[https://bitbucket.org/fdupros/ondes3d/src/master/][Ondes3d]] were
   used because this work tends to cover not only synthetic benchmarks
   which try to mimic real applications but rather use real
   applications and evaluate them. In the next session the benchmarks
   are described more detail.
** Benchmarks
Below the benchmarks used are described.

*** Intel MPI Benchmark
The Intel® MPI Benchmarks perform a set of MPI performance
measurements for point-to-point and global communication operations
for a range of message sizes. The generated benchmark data fully
characterizes:
- Performance of a cluster system, including node performance, network
  latency, and throughput.
- Efficiency of the MPI implementation used.
There are several benchmarks included in this set, and in this
evaluation, only the MPI1 PingPong application was used to measure
interconnect latency and throughput.
*** NAS Parallel Benchmarks
The NAS Parallel Benchmarks (NPB) are a small set of programs designed
to help evaluate the performance of parallel supercomputers. The
benchmarks are derived from computational fluid dynamics (CFD)
applications. Problem sizes in NPB are predefined and indicated as
different classes.

In this evaluation, the original set of benchmarks from the NPB suite,
consisting of five kernels and three pseudo-applications were used
with the Messsage Passing Interface (MPI) parallel implementation.

Five Kernels:
- *IS* - Integer Sort, random memory access.
- *EP* - Embarrassingly Parallel.
- *CG* - Conjugate Gradient, irregular memory access and communication.
- *FT* - Discrete 3D fast Fourier Transform, all-to-all communication.
- *MG* - Multi-Grid on a sequence of meshes, long- and short-distance
  communication, memory intensive.

Three pseudo-applications: 
- *BT* - Block Tri-diagonal solver.
- *SP* - Scalar Penta-diagonal solver.
- *LU* - Lower-Upper Gauss-Seidel solver.

They were executed with 128 processes on 4 nodes, 32 processes per
node, in the case of IS, EP, CG, FT, MG, and LU (power-of-two). Since
BT and SP require the number of processes to be a square root, 144
processes were used, with 36 processes in each node.

*** Inbalanced Benchmark
Imbalance Benchmark (ImbBench) is a set of MPI-based applications,
created by the Ph.D. Student Eduardo Roloff, that simulate several
behaviors in terms of process loads. ImbBench was designed with the
heterogeneity of the cloud in mind, and its goal is to help the user
to choose the most suitable configuration to execute an application in
the cloud. ImbBench distributes the load among all the available
processes according to a preselected imbalance pattern.

ImbBench has a set of microbenchmarks and parameters to benchmark CPU
and Memory. In this evaluation, it was used both CPU and Memory
benchmarks with the 8Level pattern of imbalance and the Rand and BST
microbenchmarks, respectively.

*** Alya
Alya is a high performance computational mechanics code to solve
complex coupled multi-physics / multi-scale / multi-domain problems,
which are mostly coming from the engineering realm. Among the
different physics solved by Alya we can mention:
incompressible/compressible flows, non-linear solid mechanics,
chemistry, particle transport, heat transfer, turbulence modeling,
electrical propagation, etc.

*** Ondes3d
It is an application used for seismic wave propagation simulation,
developed by the French Department of Geological and Mineral Research
(Bureau de Recherches Géologiques et Minières-BRGM). The principle of
operation of this application is the use of equations of elastodynamic
physics to represent the seismic waves, and the use of the finite
difference method to solve these equations. Ondes3D presents
characteristics such as load unbalance and frequent communication
between processes.

For the implementation of Ondes3D, the configuration parameters used
were defined by the SISHUAN simulation, a real earthquake that
occurred in China in 2008, with magnitude 8.0 on the Richter scale
using the pure MPI version.

** How to Reproduce it
To reproduce this project, the first step is to to clone the git repository in
the *$HOME* of the desired cluster of servers. 

#+begin_src shell :results output :exports both
cd $HOME; git clone https://github.com/andermm/CMP223
#+end_src

Here it is assumed that your HOME directory is exported with NFS. If
you don't have the NFS configured, [[https://www.digitalocean.com/community/tutorials/how-to-set-up-an-nfs-mount-on-ubuntu-16-04][here]] is a tutorial to do it. If
this step is ok, go ahead to the other topics.
** Software Installation
To execute the experiments, some required packages need to
be installed. They are:
- openmpi-bin - High performance message passing library (mpiexec).
- libopenmpi-dev - High performance message passing library (mpicc and
  mpifort).
- gfortran - GNU Fortran 95 compiler.
- cmake - Cross-platform, open-source make system.
- pajeng - Space-time view and associated tools for Paje trace files.

If your OS is Debian based and you have permission to install new
packages, you can install the required packages using the [[SH/software_install.sh][Software
Installer]] script. On the other hand, if you do not have permission to
do so, you can use the Spack package manager. The full tutorial on how
to use Spack can be seen [[https://spack-tutorial.readthedocs.io/en/latest/][here]].
** System Information 
   To collect the information of all nodes used in the evaluation, it
   was used the [[SH/sys_info_collect.sh][System Information Collect]] script, which creates the
   [[LOGS/env_info.org][System Information]] log output with ORG extension. The script
   is executes automatically in the execution script.

** Network Infrastructure Information
Each node has a Mellanox MT27600 Channel Adapter (CA) configured for
the InfiniBand 56 Gb/s 4X FDR ConnectX-3 with firmware version
10.16.1038 and OFED version 4.6-1.0.1.1. All nodes are interconnected
through a Mellanox SX6036 FDR and a generic de 1 Gbps switch.

** Design of Experiments
   To execute the benchmarks without any bias, the DoE.base library
   was used ([[R/DoE.R][Design of Experiments]] script) to create the Design of
   Experiments, which generates two CSV files. In DoE, two factors,
   apps and interface were used, with 30 randomized replications
   totalizing distinct 1170 (13*3*30) executions in the case of the
   execution and 1 randomized replication totalizing 36 (13*3)
   executions in the case of characterization.

- Factor 1 - Apps: The applications name totaling 13 (~bt.D.x~, ~ep.D.x~,
  ~cg.D.x~, ~mg.D.x~, ~lu.D.x~, ~sp.D.x~, ~is.D.x~, ~ft.D.x~, ~imb_memory~, ~imb_CPU~,
  ~ondes3d~, ~intel~, and ~Alya.x~).
- Factor 2 - Interface - The network interface name, totaling 3 (~eth~,
  ~ib~, ~ipoib~).

** Bash Scripts Descriptions
- [[SH/benchmarks_exec.sh][Benchmarks Execution]]
- [[SH/benchmarks_charac.sh][Benchmarks Characterization]]
- [[SH/central.sh][Central]]
- [[SH/software_install.sh][Software Installation]]
- [[SH/sys_info_collect.sh][System Information Collect]]

** Experiments Execution
 The execution script was made to be used in a cluster
 with Slurm job scheduler with the *sbatch* command. However, in the InfiniBand experiments, an
 error was reported regarding IB memory limitations. To overcome this
 problem, a central script was created ([[/BATCH/central.sh]]), which allocated the nodes in
 the cluster using the command ~salloc -p hype -N 4 -J JOB -t 72:00:00~
 and calls for the execution script ([[/BATCH/nas.batch]]) to be
 executed as a normal bash script. As the experiments use two distint machine files
 ([[./LOGS/nodes_power_of_2]] and [[./LOGS/nodes_square_root]]) they are not
 automatically created in the execution script. If you are reproducing
 the experiments in an enviroment without a job scheduler manager,
 just execute the script ([[/BATCH/nas.sbatch]]) as a normal bash
 file.
** Graphical Analysis 
After the conclusion of the experiments, in this topic, graphs
containing the execution time of the applications were created. The
first step is to read the CSV file. Next, four graphs are created
according to the number of processes, in which BT and SP have 121, and
according to a similar execution time range 
*** Read CSV
#+begin_src R :results output :session *R* :exports both
options(crayon.enabled=FALSE)
library("tidyverse")
df <- read_csv("LOGS/npb.02-11-2019.21h37m51s.csv")
 df$apps=toupper(df$apps) 
    
    df %>%
      group_by(apps,interface) %>%
      summarise(
        mean=mean(time),
        sd=sd(time),
        se=sd/sqrt(n()),
        N=n()) %>%
      arrange(apps,interface) -> newdf
newdf
#+end_src

#+RESULTS:
#+begin_example

── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──
✔ ggplot2 3.2.1     ✔ purrr   0.3.2
✔ tibble  2.1.3     ✔ dplyr   0.8.3
✔ tidyr   1.0.0     ✔ stringr 1.4.0
✔ readr   1.3.1     ✔ forcats 0.4.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()

Parsed with column specification:
cols(
  apps = col_character(),
  interface = col_character(),
  time = col_double()
)

# A tibble: 24 x 6
# Groups:   apps [8]
   apps  interface   mean     sd     se     N
   <
<
     <
 <
 <
<int>
 1 BT    ETH        558.   2.01  0.366     30
 2 BT    IB         414.   1.22  0.222     30
 3 BT    IPoIB      690.  10.5   1.92      30
 4 CG    ETH        771.   3.49  0.637     30
 5 CG    IB         184.   1.97  0.360     30
 6 CG    IPoIB      182.   2.44  0.445     30
 7 EP    ETH         37.5  1.14  0.209     30
 8 EP    IB          37.3  0.222 0.0406    30
 9 EP    IPoIB       38.2  1.91  0.349     30
10 FT    ETH       1584.   0.726 0.132     30
# … with 14 more rows
#+end_example
*** BT-SP(121 Procs)
#+begin_src R :results output graphics :file PLOTS/BT_SP.png :exports both :width 600 :height 400 :session *R*
ggplot(newdf[newdf$apps %in% c("BT", "SP"), ] , aes(x=apps, y=mean, fill=interface)) +
    geom_bar(stat="identity", position = "dodge", width = 0.2) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.05, position = position_dodge(.2)) +
    theme_minimal() +
    scale_fill_manual(values=c("#006dff", "#5ca3ff", "#b2d3ff"), name="Network\nInterface",
    breaks=c("ETH", "IB", "IPoIB"), labels=c("Ethernet", "InfiniBand", "IP-over-IB")) +
    theme(legend.position = c(0.15, 0.85), legend.background = element_rect(color = "black",
    size = 0.3, linetype = "solid"), axis.text=element_text(size=12), 
    axis.title=element_text(size=12), legend.title = element_text(color = "black", size = 14),
    legend.text = element_text(color = "black", size = 12)) +
    labs(x="Application", y="Execution Time [s]")
#+end_src

#+RESULTS:
[[file:PLOTS/BT_SP.png]]

*** FT(128 Procs)
#+begin_src R :results output graphics :file PLOTS/FT.png :exports both :width 600 :height 400 :session *R* 
   ggplot(newdf[newdf$apps %in% c("FT"), ] , aes(x=apps, y=mean, fill=interface)) +
    geom_bar(stat="identity", position = "dodge", width = 0.1) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.05, position = position_dodge(.1)) +
    theme_minimal() +
    scale_fill_manual(values=c("#006dff", "#5ca3ff", "#b2d3ff"), name="Network\nInterface",
    breaks=c("ETH", "IB", "IPoIB"), labels=c("Ethernet", "InfiniBand", "IP-over-IB")) +
    theme(legend.position = c(0.15, 0.85), legend.background = element_rect(color = "black",
    size = 0.3, linetype = "solid"), axis.text=element_text(size=12), 
    axis.title=element_text(size=12), legend.title = element_text(color = "black", size = 14),
    legend.text = element_text(color = "black", size = 12)) +
    labs(x="Application", y="Execution Time [s]")
#+end_src

#+RESULTS:
[[file:PLOTS/FT.png]]

*** EP-IS-MG(128 Procs)
#+begin_src R :results output graphics :file PLOTS/EP_IS_MG.png :exports both :width 600 :height 400 :session *R* 
ggplot(newdf[newdf$apps %in% c("EP", "IS", "MG"), ] , aes(x=apps, y=mean, fill=interface)) +
    geom_bar(stat="identity", position = "dodge", width = 0.3) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.1, position = position_dodge(.3)) +
    theme_minimal() +
    scale_fill_manual(values=c("#006dff", "#5ca3ff", "#b2d3ff"), name="Network\nInterface",
    breaks=c("ETH", "IB", "IPoIB"), labels=c("Ethernet", "InfiniBand", "IP-over-IB")) +
    theme(legend.position = c(0.15, 0.85), legend.background = element_rect(color = "black",
    size = 0.3, linetype = "solid"), axis.text=element_text(size=12), 
    axis.title=element_text(size=12), legend.title = element_text(color = "black", size = 14),
    legend.text = element_text(color = "black", size = 12)) +
    labs(x="Application", y="Execution Time [s]")
#+end_src

#+RESULTS:
[[file:PLOTS/EP_IS_MG.png]]
*** CG-LU(128 Procs)
#+begin_src R :results output graphics :file PLOTS/CG_LU.png :exports both :width 600 :height 400 :session *R* 
ggplot(newdf[newdf$apps %in% c("CG", "LU"), ] , aes(x=apps, y=mean, fill=interface)) +
    geom_bar(stat="identity", position = "dodge", width = 0.2) +
    geom_errorbar(aes(ymin=mean-sd, ymax=mean+sd), width=.05, position = position_dodge(.2)) +
    theme_minimal() +
    scale_fill_manual(values=c("#006dff", "#5ca3ff", "#b2d3ff"), name="Network\nInterface",
    breaks=c("ETH", "IB", "IPoIB"), labels=c("Ethernet", "InfiniBand", "IP-over-IB")) +
    theme(legend.position = c(0.9, 0.85), legend.background = element_rect(color = "black",
    size = 0.3, linetype = "solid"), axis.text=element_text(size=12), 
    axis.title=element_text(size=12), legend.title = element_text(color = "black", size = 14),
    legend.text = element_text(color = "black", size = 12)) +
    labs(x="Application", y="Execution Time [s]")
#+end_src

#+RESULTS:
[[file:PLOTS/CG_LU.png]]

