#+TITLE: CMP223
#+AUTHOR: Anderson M. Maliszewski
#+STARTUP: overview indent
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

* Overview
This repository is for the *CMP223 - Computer System Performance*
*Analysis (2019/2)* final work, and has the entire project used to
perform it. Below are the files and folders, along with the analysis
environment and software requirements description. Mode details on how
to reproduce the experiments are in [[LabBook.org]].

* Software Requirements

* Folders and Files Description
- [[LOGS]] - This folder has the benchmarks execution logs in the CSV
  files (~apps_exec~, ~apps_charac~, ~imb_exec~, ~imb_charac~, and ~intel~),
  four machine files (~nodes_full~, ~nodes_power_of_2~, ~nodes_square_root~,
  and ~nodes_intel~) used by the MPI execution line, environment
  information about the nodes used in the experiments in an ORG file
  (~env_info~), and a folder called BACKUP that contains the
  applications logs without any modification.
- [[PLOTS]] - Here are the graphs of the analysis made with the execution
  time of the applications, as well as its characterization genereted
  with R statistical language.
- [[R]] - This folder has the two Design of Experiments (DoE) (~DoE_exec~
  and ~DoE_charac~) files, used to generate the experimental design,
  which are the two CSV files (~experiment_project_exec~ and
  ~experimental_project_charac~) responsible for having random
  applications/network interface combinations using 30 execution
  replications.
- [[SH]] - Here are five bash scripts (~benchmark_exec~, ~benchmark_charac~,
  ~central~, ~software_intall~, ~sys_info_collect~) that are used to perform
  the experiments and collect system information.
- BENCHMARKS - This folder has all the five benchmarks (Alya,
  ImbBench, Intel MPI Benchmarks, Nas Parallel Benchmarks, and
  Ondes3d) subdivided into execution and characterization
  folders. This was made because the benchamarks execution used the
  default (mpicc and/or mpifort) compiler, and the benchmarks
  characterization used the Score-P and default compiler to generate
  the binaries that are traced during its execution.
- SOFTWARE - Here are the three softwares ([[https://www.vi-hps.org/projects/score-p/][Score-P]], [[https://github.com/schnorr/akypuera][Akypuera]], and
  [[https://github.com/schnorr/pajeng][PajeNG]]) used in the tracing process.
- LabBook.org - In this file is described the objective of this work,
  as well as the step-by-step execution of the entire project.

* Analysis Environment 
As mentioned before, some specific softwares are necessary to
reproduce this evaluation accurately. They are:
- Emacs and Org capable of compiling blocks of R code using Org-Babel
They are used as a project management tool to keep track, record
all the information, and  generate the graphics using blocks of
R code. A complete tutorial about how to install and configure the
mentioned software are available [[https://app-learninglab.inria.fr/gitlab/learning-lab/mooc-rr-ressources/blob/master/module2/ressources/emacs_orgmode.org][here]]. If you don't want to use this
softwares, you can copy the blocks of code and use R or RStudio to
generate the graphics.
 


