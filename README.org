#+TITLE: CMP223
#+AUTHOR: Anderson M. Maliszewski
#+STARTUP: overview indent
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+SEQ_TODO: TODO(t!) STARTED(s!) WAITING(w!) | DONE(d!) CANCELLED(c!) DEFERRED(f!)

This repository is for the *CMP223 - Computer System Performance*
*Analysis (2019/2)* final work, and has the entire project used to
perform it. Below are the files and folders, along with the analysis
environment and references. Mode details on how to reproduce the experiments are in
[[LabBook.org]].

#+BEGIN_QUOTE
*Reporting errors*: This repository has several links to self-contained
 files as well as links from the Internet, so if you try to reproduce
 it and find broken links or other problems, please tell me everything
 so that I can improve it. :)
#+END_QUOTE


* Folders and Files Description
- [[LOGS]] - This folder has the benchmark execution logs in the CSV files
  (~apps_exec~, ~apps_charac~, ~imb_exec~, ~imb_charac~, and ~intel~), four
  machine files ([[LOGS/nodes_full][nodes_full]], [[LOGS/nodes_power_of_2][nodes_power_of_2]], [[LOGS/nodes_square_root][nodes_square_root]] and
  [[LOGS/nodes_intel][nodes_intel]] used by the MPI execution line, a file containing the
  [[LOGS/env_info.org][environment information]] used in the experiments, and a folder called
  [[LOGS/BACKUP][BACKUP]] that includes the application logs without any modification.
- [[PLOTS]] - Here are the graphs of the analysis made with the execution
  time of the applications, as well as its characterization genereted
  with R statistical language.
- [[R]] - This folder has the Design of Experiments outputs (made in the
  [[LabBook.org][LabBook.org]] → Experiment Project → Design of Experiments), called
  [[R/experimental_project_exec.csv][experimental_project_exec]] and [[experimental_project_charac.csv][exeperimental_project_charac]] which are
  responsible for random applications/network interface combinations
  using 30 execution replications for normal execution (exec) and 1
  replication for characterization execution (charac).
- [[SH]] - Here are six bash scripts ([[SH/experiments_exec.sh][experiments_exec]], [[SH/experiments_charac.sh][experiments_charac]],
  [[SH/central.sh][central]], [[SH/software_install.sh][software_install]], and [[SH/sys_info_collect.sh][sys_info_collect]]) that are used to
  perform the experiments and collect system information.
- [[BENCHMARKS]] - This folder has all the four benchmarks (Alya,
  ImbBench, Intel MPI Benchmarks and NAS Parallel Benchmarks)
  subdivided into execution and characterization folders. This was
  made because the benchamarks execution used the mpicc and/or mpifort
  compiler, and the benchmarks characterization used the Score-P and
  the mpicc and/or mpifort compiler to generate the binaries that are
  traced during its execution.
- [[SOFTWARE]] - Here are the three softwares ([[https://www.vi-hps.org/projects/score-p/][Score-P]], [[https://github.com/schnorr/akypuera][Akypuera]], and
  [[https://github.com/schnorr/pajeng][PajeNG]]) used in the tracing process.
- [[LabBook.org]] - In this file is described the objective of this work,
  as well as the step-by-step execution of the entire project.

* Analysis Environment 
Some specific softwares are required to reproduce this evaluation
accurately. They are Emacs and Org, and both are used as project
management tools to track, record all information, and generate
graphics using R code blocks. A complete tutorial on how to install
and configure the mentioned softwares is available [[https://app-learninglab.inria.fr/gitlab/learning-lab/mooc-rr-ressources/blob/master/module2/ressources/emacs_orgmode.org][here]]. If you do not
want to use this softwares, you can copy the blocks of code and use R
or RStudio to generate the graphics.
 
* References
+ R. Jain, [[http://www.cs.wustl.edu/~jain/books/perfbook.htm][The Art of Computer Systems Performance Analysis:
  Techniques for Experimental Design, Measurement, Simulation, and
  Modeling]], Wiley-Interscience, New York, NY, April 1991.
#+BEGIN_QUOTE
This was the base book used in the *CMP223 - Computer System
Performance Analysis (2019/2)* discipline and can be considered the
oracle for creating performance evaluations even though it is old, but
the checklist and concepts are still current today.
#+END_QUOTE
+ Legrand. Arnaud, Schnorr. Lucas, [[https://github.com/alegrand/SMPE.git][Scientific Methodology and
  Performance Evaluation for Computer Scientists]], GitHub Repository.
#+BEGIN_QUOTE
In this repository is a course called *Scientific Methodology and
Performance Evaluation for Computer Scientists*, based on several
renowned books (including Jain's). The aim is to provide the
fundamental basis for a sound scientific methodology for performance
evaluation of computer systems. This course has completely changed the
way I view and create performance evaluations, and I really recommend
it to you.
#+END_QUOTE

